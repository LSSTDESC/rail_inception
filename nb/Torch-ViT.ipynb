{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3544e064-d8a1-4897-85ac-5881d93e8343",
   "metadata": {},
   "source": [
    "# Photo-z with torch implementation of ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea35939-9889-4b90-a0fc-b5c5caba7651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca4450-2ee5-462f-aa50-bdae498d4127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "lr = 3e-4\n",
    "gamma = 0.7\n",
    "seed = 111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9314ff-8e30-4887-82c0-377498d4cde8",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c4a97-cca3-4916-9c1b-3294e2c0b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.load('/global/cfs/cdirs/lsst/groups/PZ/valentin_image_data_temp/img_30k.npy')\n",
    "z = np.load('/global/cfs/cdirs/lsst/groups/PZ/valentin_image_data_temp/z_30k.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d7624b-4246-4c69-b245-f1d86d5cb7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the images\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "scaling = []\n",
    "for i,b in enumerate(['g', 'r', 'i', 'z', 'y']):\n",
    "    sigma = 1.4826*median_abs_deviation(img[...,i].flatten())\n",
    "    scaling.append(sigma)\n",
    "    \n",
    "def preprocessing(image):\n",
    "    return np.arcsinh(img / scaling / 3. )\n",
    "\n",
    "img = preprocessing(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6f9ab-5c87-4d3d-8ba1-49065d6b505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check transformed images\n",
    "plt.imshow(img[0, ..., :3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a87af-c6c4-4f1c-8118-2ef910294bf4",
   "metadata": {},
   "source": [
    "Shapes of images in torch need to be organised differently from (number, width, height, depth) to (number, depth, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d0dc1-b9d8-478f-bbfb-6fc9be3f1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.transpose(torch.transpose(torch.tensor(img), 2,3), 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4361f-367e-4766-8f91-ea6846dae80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4425d537-7453-49c3-96b8-ba28d9aae826",
   "metadata": {},
   "source": [
    "### Loader without Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca44ab-ed1b-457a-bbe3-a4a15b285861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting train, val and test image data into tensors\n",
    "train_X_tensor = img[:15000].to(torch.float32)\n",
    "val_X_tensor = img[15000:20000].to(torch.float32)\n",
    "test_X_tensor = img[20000:].to(torch.float32)\n",
    "\n",
    "# Converting train and val labels into tensors\n",
    "train_y_tensor = torch.tensor(z[:15000]).to(torch.float32)\n",
    "val_y_tensor = torch.tensor(z[15000:20000]).to(torch.float32)\n",
    "\n",
    "# Creating train  and val tensors\n",
    "train_tensor = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "val_tensor = TensorDataset(val_X_tensor, val_y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c14f6-7436-4b30-b623-b6b42521b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset = train_tensor, batch_size=batch_size)\n",
    "val_loader = DataLoader(dataset = val_tensor, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d33d9af-c33d-4004-9818-8d2008d95d1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dcd607-5d8e-43e4-8477-ae7462b2cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    dim=256,\n",
    "    image_size=64,\n",
    "    patch_size=16,\n",
    "    num_classes=1,\n",
    "    depth=6,\n",
    "    heads=16,\n",
    "    mlp_dim=512,\n",
    "    channels=5,\n",
    "    pool='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4813459-0bc1-4256-bdea-d6cc3463bcd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350582a-65c7-4675-a658-d6a1b521e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer\n",
    "lr = 3e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0132004-2c94-4be6-aa54-42990e001bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "losses = {'train':[],\n",
    "          'val':[]}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for data, label in tqdm(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data).squeeze()\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        #optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss / len(train_loader)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        for data, label in val_loader:\n",
    "            val_output = model(data).squeeze()\n",
    "            val_loss = criterion(val_output, label)\n",
    "            epoch_val_loss += val_loss / len(val_loader)\n",
    "            \n",
    "    scheduler.step()\n",
    "\n",
    "    losses['train'].append(epoch_loss.detach()), losses['val'].append(epoch_val_loss.detach())\n",
    "    print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - val_loss : {epoch_val_loss:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bda5b3-f016-4c3b-9bd6-1e4a4dea32d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model): \n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'\\n Number of parameters: {count_parameters(model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399bfb31-2bb9-403d-a708-b7642eb331f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = np.arange(2, epochs+1)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(epoch, losses['train'][1:], label='Train')\n",
    "plt.plot(epoch, losses['val'][1:], label='val')\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.legend(loc='upper right', prop = { \"size\": 16 });"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25b23d-1848-4e7d-a1a8-069263e21c8a",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1c23d-6304-4934-af3d-3518e32dc589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction\n",
    "preds = model(test_X_tensor).detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036f016-b46d-4bc9-b267-ad91a84d89ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics results\n",
    "dz, pred_bias, smad, out_frac = metrics(z[20000:], preds)\n",
    "print_metrics(pred_bias, smad, out_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d48362-6654-4506-9553-13e1db537060",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(z[20000:], preds, pred_bias, out_frac, smad, 'Vision Transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e59e8-33e0-4783-bbbd-07c6ce9f25bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
