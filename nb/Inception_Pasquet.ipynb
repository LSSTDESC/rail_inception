{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499ba180-0d33-46cd-bf27-6fc1bceb0539",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inception model - Pasquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07fb76-65d6-4af2-b682-149b0e6df55b",
   "metadata": {},
   "source": [
    "Here, we explore the inception model [from Pasquet et al. article.](https://arxiv.org/pdf/1806.06607.pdf):\n",
    "\n",
    "- First the regular model \n",
    "- Then the augmented model with the galacatif redenning\n",
    "- Finaly, an exploration of probability ditributions as output of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c45857-fe14-4cc3-ba64-b46cfbc2c6f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4664b-6005-41ec-bbfc-bc0743d2e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Checking for GPU access\n",
    "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "  print('WARNING: GPU device not found.')\n",
    "else:\n",
    "  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540ec98-d27a-44bc-9296-9b7be56f5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scipy.stats import median_abs_deviation\n",
    "from tensorflow.keras.metrics import mse\n",
    "\n",
    "from tools import *\n",
    "from model_inception import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ffded6-4318-40a9-bd73-b09bdd206730",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4985322-fc87-4e1e-a72b-aebd7f094916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is stored in the following repo\n",
    "%ls /global/cfs/cdirs/lsst/groups/PZ/valentin_image_data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c4b1b-e2eb-48ba-81e4-dae5b2d9deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use here directly the two numpy files actually extracted from 'download'\n",
    "img_path = '/global/cfs/cdirs/lsst/groups/PZ/valentin_image_data_temp/img_30k.npy'\n",
    "z_path = '/global/cfs/cdirs/lsst/groups/PZ/valentin_image_data_temp/z_30k.npy'\n",
    "\n",
    "img = np.load(img_path)\n",
    "z = np.load(z_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf9e55-a059-4805-8e95-7eb3ac952e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "scaling = []\n",
    "for i in range(img.shape[-1]):\n",
    "    sigma = 1.4826*median_abs_deviation(img[...,i].flatten())\n",
    "    scaling.append(sigma)\n",
    "\n",
    "img = np.arcsinh(img / scaling / 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1c056c-9004-4745-a583-0e5508d6046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, validation & test split\n",
    "data = {}\n",
    "data['train'] = [img[:15000,...], z[:15000]]\n",
    "data['val'] = [img[15000:20000,...], z[15000:20000]]\n",
    "data['test'] = [img[20000:,...], z[20000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f7a49-c6dd-4883-96cc-f8b265d4fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check transformed images\n",
    "plt.imshow(data['train'][0][0, ..., :3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633f4542-364e-4765-b78a-cc774d362367",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regular Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b40ff3-6847-4355-9dad-cc76f17f3a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_tf2()\n",
    "model.compile(optimizer='adam', loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b4b43-98f9-4ed5-9395-b81350c9c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7ee74-f96a-4d90-8173-72b00068118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8561d-5689-4fb9-b244-3842e33bb6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule\n",
    "LEARNING_RATE=0.001\n",
    "LEARNING_RATE_EXP_DECAY=0.9\n",
    "lr_decay = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch,\n",
    "    verbose=True)\n",
    "\n",
    "# Tensoboard tracking\n",
    "#tb_callback = tf.keras.callbacks.TensorBoard('./logs/inception', update_freq='batch')\n",
    "\n",
    "model.compile(optimizer='adam', loss=mse)\n",
    "history = model.fit(x = data['train'][0], \n",
    "          y = data['train'][1],\n",
    "          batch_size = 64,\n",
    "          validation_data=(data['val'][0], data['val'][1]),\n",
    "          steps_per_epoch=len(data['train'][0])//64,\n",
    "          epochs=25,\n",
    "          callbacks=[lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8a835-0c2f-400f-9364-d635df478266",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history, 'Inception Model Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a427684-fdd7-46e9-bed0-42a222d149e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prediction\n",
    "preds = model.predict(data['test'][0]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ffc37-1417-4b60-8799-61e6bf6b6d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics results\n",
    "dz, pred_bias, smad, out_frac = metrics(data['test'][1], preds)\n",
    "print_metrics(pred_bias, smad, out_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9b18e-1b0f-4ecd-849d-6ecf2cfeb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(data['test'][1], preds, pred_bias, out_frac, smad, 'Inception')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e06ce-eb0f-45ae-b67c-80b684a433e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Residuals analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b92ff-f39e-4e33-b65a-fbe89547f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta_z histogram\n",
    "plt.hist(dz, bins=50, color='orange')\n",
    "plt.title('$ \\Delta z$ Distribution', fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b8cb2-de77-43f4-bea2-c502d3594b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta_z repartition with spectroscopic redshift\n",
    "plt.scatter(data['test'][1], dz, s=1)\n",
    "plt.axhline(0.05, color='r', linestyle='--')\n",
    "plt.axhline(-0.05, color='r', linestyle='--')\n",
    "plt.xlabel('Spectroscopic Redshift' , fontsize=14)\n",
    "plt.ylabel('$ \\Delta z$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e56ac2-0da4-43ed-8e50-4782be420d0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Same model with larger training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd6e41-ff0d-4005-9764-c88d9c84a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /global/cfs/cdirs/lsst/groups/PZ/valentin_image_data_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60533faf-8bba-4340-b173-abe72ca42f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = np.load('/global/cfs/cdirs/lsst/groups/PZ/valentin_image_data_temp/download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc8967-2057-4c52-a8ed-8f1f96d4d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.DataFrame(data_full[\"labels\"][:60000]).z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4eb77e-10ef-44fb-81d3-19d8c400d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = data_full[\"cube\"][:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6b474f-b7cf-42a8-b2d3-8d9e2b251bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = []\n",
    "for i in range(img.shape[-1]):\n",
    "    sigma = 1.4826*median_abs_deviation(img[...,i].flatten())\n",
    "    scaling.append(sigma)\n",
    "\n",
    "img = np.arcsinh(img / scaling / 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36c43fb-ec6d-4cfa-b5a3-8ddd4d8aef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_data = {}\n",
    "large_data['train'] = [img[:40000,...], z[:40000]]\n",
    "large_data['val'] = [img[40000:50000,...], z[40000:50000]]\n",
    "large_data['test'] = [img[50000:,...], z[50000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49539ca-0fda-42d0-ab84-4f4c708ea107",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_model = model_tf2()\n",
    "\n",
    "large_model.compile(optimizer='adam', loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87484cc-bbb7-475f-9e73-6521d24cf6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f3242c-1c80-41d8-a9ac-37e530d7cfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule\n",
    "LEARNING_RATE=0.001\n",
    "LEARNING_RATE_EXP_DECAY=0.9\n",
    "lr_decay = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch,\n",
    "    verbose=True)\n",
    "\n",
    "# Tensoboard tracking\n",
    "#tb_callback = tf.keras.callbacks.TensorBoard('./logs/inception', update_freq='batch')\n",
    "\n",
    "\n",
    "large_history = large_model.fit(x = large_data['train'][0], \n",
    "          y = large_data['train'][1],\n",
    "          batch_size = 64,\n",
    "          validation_data=(large_data['val'][0], large_data['val'][1]),\n",
    "          steps_per_epoch=len(large_data['train'][1])//64,\n",
    "          epochs=25,\n",
    "          callbacks=[lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3058e-60d9-4986-acb1-a346b8ad846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(large_history, 'Large Inception Model Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583de74-6fe9-4795-b50f-ee7b4b4fea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "preds = large_model.predict(large_data['test'][0]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a05be-8798-4930-8c7b-027ed851f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz, pred_bias, smad, out_frac = metrics(large_data['test'][1], preds)\n",
    "print_metrics(pred_bias, smad, out_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4916f-a381-4e1e-963a-5601c4fd7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(large_data['test'][1], preds, pred_bias, out_frac, smad, 'Large Inception')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c6e2aa-42d5-4521-988a-837fe8ababbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extra feature: Galactic Reddening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f83ded-8ea6-40c6-8ceb-6b8454404499",
   "metadata": {},
   "source": [
    "One can add extra features and concatenate them after the convolution part of the inception model to improve the estimated redshifts. In Pasquet's article, they propose to use the galactic redenning to do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629bc659-9f14-4627-85f6-6c739b4c0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_download = np.load('/global/cfs/cdirs/lsst/groups/PZ/valentin_image_data_temp/download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45bb6f-a4c7-4fbf-9bbd-4d1c9ac3a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pd.DataFrame(data_download[\"labels\"][:30000] )\n",
    "cat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5641f-953a-49a3-8691-46f6b0df092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EBV = cat.EBV\n",
    "EBV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0355c-7956-47f8-97f5-096918b56fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(EBV, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40468c4c-1c31-4382-ac5c-41338e3fcdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, val and test taking 1/2, 1/4 and 1/4 respectively\n",
    "EBV_train = EBV[:15000]\n",
    "EBV_val = EBV[15000:20000]\n",
    "EBV_test = EBV[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cc46a-c4aa-4e87-9b51-351e88424909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ebv = model_tf2(with_ebv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde7dbb5-4130-4863-9c38-3c02349d5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ebv.compile(optimizer='adam', loss=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8d741-bf28-4486-81f8-65691a7db04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule\n",
    "LEARNING_RATE=0.001\n",
    "LEARNING_RATE_EXP_DECAY=0.9\n",
    "lr_decay = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch,\n",
    "    verbose=True)\n",
    "\n",
    "# Tensoboard tracking\n",
    "#tb_callback = tf.keras.callbacks.TensorBoard('./logs/inception_w_EBV', update_freq='batch')\n",
    "\n",
    "history_ebv = model_ebv.fit(x = [data['train'][0], EBV_train], \n",
    "          y = data['train'][1],\n",
    "          batch_size = 64,\n",
    "          validation_data=([data['val'][0], EBV_val], data['val'][1]),\n",
    "          steps_per_epoch=len(data['train'][0])//64,\n",
    "          epochs=25,\n",
    "          callbacks=[lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697696e-4c02-435b-a681-d344841e35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history_ebv, 'Inception Model with EBV Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8320af-ae62-4748-be87-8f2ccc7e9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "preds_ebv = model_ebv.predict([data['test'][0], EBV_test]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf0b6a-7e69-40f1-9d45-910738d05d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz, pred_bias, smad, out_frac = metrics(data['test'][1], preds_ebv)\n",
    "print_metrics(pred_bias, smad, out_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7dff56-8760-4661-bd2a-99518667206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(data['test'][1], preds_ebv, pred_bias, out_frac, smad, 'Inception with extra feature EBV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fd4d94-b1dd-4be7-bdb9-2e84fa3bb7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(data['test'][1], preds_ebv, 64, range=[[0,0.7],[0,0.7]], cmap='gist_stern'); \n",
    "plt.gca().set_aspect('equal');\n",
    "plt.plot([0,0.7],[0,0.7],color='r')\n",
    "plt.xlabel('Spectroscopic Redshift')\n",
    "plt.ylabel('Predicted Redshift');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d1cc5-0d9c-4245-95eb-3578ca35d42e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Probability Distribitions Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5572f1c-7392-4f3c-bc28-bc8b112536f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_tf2(output_distrib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d3d89-102c-4dfb-8298-00615fc684c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "negloglik = lambda y, p_y: -p_y.log_prob(y)\n",
    "model.compile(optimizer='adam', loss=negloglik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcadd7d-a064-40c2-93a0-618e3ecaefb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d5188-5779-4198-966e-6a91ba8d5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule\n",
    "LEARNING_RATE=0.001\n",
    "LEARNING_RATE_EXP_DECAY=0.9\n",
    "lr_decay = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch,\n",
    "    verbose=True)\n",
    "\n",
    "# Tensoboard tracking\n",
    "#tb_callback = tf.keras.callbacks.TensorBoard('./logs/inception', update_freq='batch')\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(x = data['train'][0], \n",
    "              y = data['train'][1],\n",
    "              batch_size = 64,\n",
    "              validation_data=(data['val'][0], data['val'][1]),\n",
    "              steps_per_epoch=len(data['train'][0])//64,\n",
    "              epochs=10,\n",
    "              callbacks=[lr_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56ea66-7069-4aef-9e84-7d0949b53eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history, 'Inception Model with distribution output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5805f3a-60de-46e7-91d9-002638383c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beware: differences between model() and model.predict()\n",
    "# The first gives a distribution and the second an array\n",
    "\n",
    "yhat = model(np.reshape(data['test'][0][0], (1, 64, 64, 5)))\n",
    "assert isinstance(yhat, tfd.Distribution)\n",
    "\n",
    "yhat_ = model.predict(np.reshape(data['test'][0][0], (1, 64, 64, 5)))\n",
    "assert isinstance(yhat_, np.ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2506f-454f-4aa8-b4a3-660f6fe9a817",
   "metadata": {},
   "source": [
    "But what we want here are distributions as outputs and not just arrays. So we need to use the call method from model and the predict method. The problem is that with the call methods, memory issues can arise. For this reason, the predictions need to be performed on batched and not on the full test set at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abf65c-a323-4751-a372-a6e1d890765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of batch of distributions by calling the model on batches of images\n",
    "preds_distrib = []\n",
    "batch_size = 1000\n",
    "start = 0\n",
    "length = len(data['test'][0])\n",
    "while start + batch_size <length:\n",
    "    preds_distrib.append(model(np.reshape(data['test'][0][start:start+batch_size], (batch_size, 64, 64, 5))))\n",
    "    start += batch_size\n",
    "preds_distrib.append(model(np.reshape(data['test'][0][start:], (length - start, 64, 64, 5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc46dcd7-6ee6-4dc1-aa1c-8e09ae80a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_distrib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1ebc9-fd0c-4b66-874b-74a4c5a55fc3",
   "metadata": {},
   "source": [
    "The goal is to be able to have the predicted distributions directly on the whole test set\n",
    "\n",
    "It is not the case yet, so rest of the code is with 'preds', which is just a sample of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719526f3-6f33-4775-85c1-f949de1e08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the prior\n",
    "\n",
    "import scipy.stats\n",
    "hist = np.histogram(data['train'][1], 64)\n",
    "prior = scipy.stats.rv_histogram(hist)\n",
    "\n",
    "plt.hist(data['train'][1], 100, density=True);\n",
    "x = np.linspace(0, data['train'][1].max(), 100)\n",
    "plt.plot(x, prior.pdf(x));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6781bd-aa8d-4ae5-aa87-90ad434f530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns the distribution q(z | x) for all clusters\n",
    "z = np.linspace(0,0.7,100)\n",
    "logps = []\n",
    "logps_local = []\n",
    "for i in range(len(z)):\n",
    "    for k in range(len(preds_distrib)):\n",
    "        logps_local.append(preds_distrib[k].log_prob(z[i]).numpy())\n",
    "    ## Here flatten the list\n",
    "    flat_list = []\n",
    "    for sublist in logps_local:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "    logps.append(flat_list)\n",
    "    logps_local = []\n",
    "logps = np.stack(logps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d88a99-34d3-40ce-91c0-9691c6537e3b",
   "metadata": {},
   "source": [
    "The posterior can be ploted under the training prior or under a flat prior by dividing the posterior by the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ae777-44af-42ec-92a6-1ceb957c6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.figure()\n",
    "    plt.plot(z, np.exp(logps[:,i]), label='posterior under training prior')\n",
    "    plt.plot(z, np.exp(logps[:,i])/prior.pdf(z), label='posterior under flat prior')\n",
    "    plt.axvline(data['test'][1][i], color='m', label='True value')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28a720f-ded5-4f74-8b4d-f7139f3aca33",
   "metadata": {},
   "source": [
    "To evalute the results of distribution outputs, we can try to extract a point estimate of them and compare with previous results: \n",
    "- Mean \n",
    "- Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c572c43-26b8-4e53-9f04-d6ece4361ee4",
   "metadata": {},
   "source": [
    "#### Mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a4d1d-7691-4b24-81df-1779821949f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With training prior\n",
    "mean_preds = np.squeeze([preds_distrib[k].mean() for k in range(10)]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826b6f1-78ce-4808-9cdf-ff73eb18bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz, pred_bias, smad, out_frac = metrics(data['test'][1], mean_preds)\n",
    "print_metrics(pred_bias, smad, out_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87cdbfa-660f-4dad-ab5d-9978be73fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(data['test'][1], mean_preds, pred_bias, out_frac, smad, 'Inception - Mean Point Estimate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc52b6-dde3-425d-be10-3ea984868478",
   "metadata": {},
   "source": [
    "#### Mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbf766-e70a-4c7f-87d3-26f33432a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With training prior\n",
    "mode_preds = z[np.exp(logps).argmax(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46822a8c-b2ea-4b49-9b0f-675c9b59acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dz, pred_bias, smad, out_frac = metrics(data['test'][1], mode_preds)\n",
    "print_metrics(pred_bias, smad, out_frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acba811-4bae-48f9-878f-36791c3c61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(data['test'][1], mode_preds, pred_bias, out_frac, smad, 'Inception - Mean Point Estimate')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f6360d0acf45bf0b1d3617497dbdfbcbdfb4334b88a4110538877fe5548438f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
